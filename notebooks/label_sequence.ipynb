{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/10 03:31:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master('local[8]')\n",
    "    .config('spark.driver.memory', '16g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.read.parquet('../data/amex-default-prediction/train_data')\n",
    "train_labels = spark.read.parquet('../data/amex-default-prediction/train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|         customer_ID|target|\n",
      "+--------------------+------+\n",
      "|0000099d6bd597052...|   0.0|\n",
      "|00000fd6641609c6e...|   0.0|\n",
      "|00001b22f846c82c5...|   0.0|\n",
      "|000041bdba6ecadd8...|   0.0|\n",
      "|00007889e4fcd2614...|   0.0|\n",
      "|000084e5023181993...|   0.0|\n",
      "|000098081fde4fd64...|   0.0|\n",
      "|0000d17a1447b25a0...|   0.0|\n",
      "|0000f99513770170a...|   1.0|\n",
      "|00013181a0c5fc8f1...|   1.0|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         customer_ID|count|\n",
      "+--------------------+-----+\n",
      "|0012e41fe6caa3ba3...|    1|\n",
      "|00582971eb87cf0bb...|    1|\n",
      "|00dbda372d97f2357...|    1|\n",
      "|00dc3b4e9f8f3b114...|    1|\n",
      "|0119d38628b2cbdd0...|    1|\n",
      "|0125e11ed5c94d63c...|    1|\n",
      "|01439ee3abf1b4552...|    1|\n",
      "|014acecc038c204f2...|    1|\n",
      "|01500e2a9f82cfab4...|    1|\n",
      "|0152860993fe1a7b1...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_labels.groupBy('customer_ID').count().orderBy('count', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 03:31:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_136</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00403d34ba512d4c72dc6fd805b84498d0dcadcfbb58ca...</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>0.791787</td>\n",
       "      <td>0.683767</td>\n",
       "      <td>0.035230</td>\n",
       "      <td>1.004302</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00b50b0ab89bfb8ef1a5942b18bea9994d0c3207131e56...</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>0.943163</td>\n",
       "      <td>0.529789</td>\n",
       "      <td>0.175606</td>\n",
       "      <td>1.004728</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.298369</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008706</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004cf587f8db6d3edb18bce27c6c4855e7c117ef8b2501...</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>0.825849</td>\n",
       "      <td>0.009611</td>\n",
       "      <td>0.022143</td>\n",
       "      <td>1.005095</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.152467</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0216684b0b1faf80959e5c9c8e14c80e2b10eb851b573b...</td>\n",
       "      <td>2017-09-21</td>\n",
       "      <td>0.848227</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.815999</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01a279b5e7dafb8970911f74da31f0a1837af0709dfc49...</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>0.901389</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.007411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.001309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0071b1b3cb9efec4a628fc21ac143ef92d0fe6b58dbf71...</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>0.859257</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>1.006282</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.416519</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.005012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01ae75485b7d790e0adcdc8d60dfdf5898b2dbdf67384d...</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>0.326465</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.077844</td>\n",
       "      <td>0.254774</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.788595</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.002814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>003b8acbe4a0440cc0793413cb667ff2aea374ff675c67...</td>\n",
       "      <td>2017-12-23</td>\n",
       "      <td>0.881174</td>\n",
       "      <td>0.239852</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>1.000403</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.118295</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.007551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0178e40e713acd4367d67a0b5d4451753edb7df39ea7b7...</td>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>0.275334</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.814972</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.005009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01b09a1d445a311a100c90f3a4a3088127bdcc96b99b79...</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>0.958853</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>0.815809</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009705</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID         S_2       P_2  \\\n",
       "0  00403d34ba512d4c72dc6fd805b84498d0dcadcfbb58ca...  2018-03-21  0.791787   \n",
       "1  00b50b0ab89bfb8ef1a5942b18bea9994d0c3207131e56...  2017-07-29  0.943163   \n",
       "2  004cf587f8db6d3edb18bce27c6c4855e7c117ef8b2501...  2017-10-25  0.825849   \n",
       "3  0216684b0b1faf80959e5c9c8e14c80e2b10eb851b573b...  2017-09-21  0.848227   \n",
       "4  01a279b5e7dafb8970911f74da31f0a1837af0709dfc49...  2017-11-28  0.901389   \n",
       "5  0071b1b3cb9efec4a628fc21ac143ef92d0fe6b58dbf71...  2017-10-08  0.859257   \n",
       "6  01ae75485b7d790e0adcdc8d60dfdf5898b2dbdf67384d...  2017-10-14  0.326465   \n",
       "7  003b8acbe4a0440cc0793413cb667ff2aea374ff675c67...  2017-12-23  0.881174   \n",
       "8  0178e40e713acd4367d67a0b5d4451753edb7df39ea7b7...  2017-05-13  0.275334   \n",
       "9  01b09a1d445a311a100c90f3a4a3088127bdcc96b99b79...  2017-06-23  0.958853   \n",
       "\n",
       "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
       "0  0.683767  0.035230  1.004302  0.003275  0.101050  0.008144  0.003354  ...   \n",
       "1  0.529789  0.175606  1.004728  0.004517  0.298369  0.003098  0.003244  ...   \n",
       "2  0.009611  0.022143  1.005095  0.004939  0.152467  0.001338  0.001610  ...   \n",
       "3  0.008164  0.002402  0.815999  0.002968       NaN  0.005372  0.003133  ...   \n",
       "4  0.004876  0.008624  0.819405  0.006558       NaN  0.007316  0.004203  ...   \n",
       "5  0.001360  0.005847  1.006282  0.000465  0.416519  0.009725  0.009772  ...   \n",
       "6  0.006871  0.077844  0.254774  0.008865  0.788595  0.008406  0.043803  ...   \n",
       "7  0.239852  0.036496  1.000403  0.015892  0.118295  0.008450  0.000240  ...   \n",
       "8  0.006802  0.003530  0.814972  0.008282       NaN  0.009127  0.006758  ...   \n",
       "9  0.003488  0.007762  0.815809  0.000493       NaN  0.009705  0.008903  ...   \n",
       "\n",
       "   D_136  D_137  D_138     D_139     D_140     D_141  D_142     D_143  \\\n",
       "0    NaN    NaN    NaN  0.006559  0.002691  0.003431    NaN  0.000187   \n",
       "1    NaN    NaN    NaN  0.005845  0.005304  0.008459    NaN  0.008706   \n",
       "2    NaN    NaN    NaN  0.005062  0.009416  0.008604    NaN  0.005272   \n",
       "3    NaN    NaN    NaN  0.006897  0.002919  0.009751    NaN  0.003399   \n",
       "4    NaN    NaN    NaN  0.002448  0.003578  0.007411    NaN  0.003352   \n",
       "5    NaN    NaN    NaN  0.000938  0.007070  0.007457    NaN  0.006416   \n",
       "6    NaN    NaN    NaN  0.007373  0.001529  0.005971    NaN  0.000473   \n",
       "7    NaN    NaN    NaN  0.001219  0.005738  0.003018    NaN  0.000663   \n",
       "8    NaN    NaN    NaN  0.001818  0.003369  0.008177    NaN  0.003823   \n",
       "9    NaN    NaN    NaN  0.008910  0.000923  0.008609    NaN  0.002536   \n",
       "\n",
       "      D_144     D_145  \n",
       "0  0.000034  0.002045  \n",
       "1  0.005755  0.001480  \n",
       "2  0.004203  0.003022  \n",
       "3  0.001608  0.002856  \n",
       "4  0.009872  0.001309  \n",
       "5  0.002880  0.005012  \n",
       "6  0.009077  0.002814  \n",
       "7  0.008475  0.007551  \n",
       "8  0.004214  0.005009  \n",
       "9  0.000206  0.008547  \n",
       "\n",
       "[10 rows x 190 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\n",
    "def diff_days(list_of_dates):\n",
    "    list_of_dates = sorted(list_of_dates)\n",
    "    out = []\n",
    "    for i in range(1, len(list_of_dates)):\n",
    "        out.append((list_of_dates[i] - list_of_dates[i-1]) / timedelta(days=1))\n",
    "    return out\n",
    "\n",
    "metrics_aggregated_by_id = (\n",
    "    train_data\n",
    "    .groupBy('customer_ID')\n",
    "    .agg(\n",
    "        F.collect_list('S_2').alias('S_2_list'),\n",
    "        F.sum(F.lit(1)).alias('num_rows'),\n",
    "        F.min('S_2').alias('min_date'),\n",
    "        F.max('S_2').alias('min_date'),\n",
    "    )\n",
    "    .withColumn('diff_days', F.udf(diff_days, ArrayType(FloatType()))('S_2_list'))\n",
    "    .drop('S_2_list')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probabilities</th>\n",
       "      <th>diff_days_quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    probabilities  diff_days_quantiles\n",
       "0        0.000000                  1.0\n",
       "1        0.052632                 12.0\n",
       "2        0.105263                 17.0\n",
       "3        0.157895                 20.0\n",
       "4        0.210526                 24.0\n",
       "5        0.263158                 26.0\n",
       "6        0.315789                 28.0\n",
       "7        0.368421                 29.0\n",
       "8        0.421053                 30.0\n",
       "9        0.473684                 30.0\n",
       "10       0.526316                 31.0\n",
       "11       0.578947                 31.0\n",
       "12       0.631579                 31.0\n",
       "13       0.684211                 33.0\n",
       "14       0.736842                 35.0\n",
       "15       0.789474                 38.0\n",
       "16       0.842105                 41.0\n",
       "17       0.894737                 45.0\n",
       "18       0.947368                 49.0\n",
       "19       1.000000                392.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "probabilities = list(np.linspace(0, 1, num=20))\n",
    "diff_days_quantiles = (\n",
    "    metrics_aggregated_by_id\n",
    "    .withColumn('diff_days', F.explode('diff_days'))\n",
    "    .approxQuantile(col='diff_days', probabilities=probabilities, relativeError=.01)\n",
    ")\n",
    "pd.DataFrame({\n",
    "    'probabilities': probabilities,\n",
    "    'diff_days_quantiles': diff_days_quantiles,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probabilities</th>\n",
       "      <th>num_rows_quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    probabilities  num_rows_quantiles\n",
       "0        0.000000                 1.0\n",
       "1        0.052632                 5.0\n",
       "2        0.105263                 9.0\n",
       "3        0.157895                13.0\n",
       "4        0.210526                13.0\n",
       "5        0.263158                13.0\n",
       "6        0.315789                13.0\n",
       "7        0.368421                13.0\n",
       "8        0.421053                13.0\n",
       "9        0.473684                13.0\n",
       "10       0.526316                13.0\n",
       "11       0.578947                13.0\n",
       "12       0.631579                13.0\n",
       "13       0.684211                13.0\n",
       "14       0.736842                13.0\n",
       "15       0.789474                13.0\n",
       "16       0.842105                13.0\n",
       "17       0.894737                13.0\n",
       "18       0.947368                13.0\n",
       "19       1.000000                13.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows_quantiles = metrics_aggregated_by_id.approxQuantile(col='num_rows', probabilities=probabilities, relativeError=.01)\n",
    "pd.DataFrame({\n",
    "    'probabilities': probabilities,\n",
    "    'num_rows_quantiles': num_rows_quantiles,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
