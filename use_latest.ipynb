{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: Dio.netty.tryReflectionSetAccessible\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/11 15:27:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from spark_utils import get_spark_session\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = get_spark_session()\n",
    "# run rank_by_latest.py if this doesn't exist\n",
    "train_data = spark.read.parquet('data_transformed/amex-default-prediction/train_data_latest')\n",
    "train_labels = spark.read.parquet('data/amex-default-prediction/train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert train_data.count() == train_data.select('customer_ID').distinct().count()\n",
    "assert train_labels.count() == train_labels.select('customer_ID').distinct().count()\n",
    "assert train_data.count() == train_data.join(train_labels, on='customer_ID', how='inner').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 48.3 ms, total: 292 ms\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from format_data import CATEGORICAL_VARIABLES\n",
    "from encoder import CategoricalToIntegerEncoders\n",
    "\n",
    "encs = CategoricalToIntegerEncoders(columns=CATEGORICAL_VARIABLES).fit(train_data)\n",
    "# not actually transformed yet, just fitted\n",
    "train_data_transformed = encs.transform(spark=spark, df=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/11 15:27:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 1.2 s, total: 2.35 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_pdf = encs.transform(\n",
    "    spark=spark,\n",
    "    df=train_data.join(train_labels, on='customer_ID', how='inner')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P_2, D_39, B_1, B_2, R_1, S_3, D_41, B_3, D_42, D_43, D_44, B_4, D_45, B_5, R_2, D_46, D_47, D_48, D_49, B_6, B_7, B_8, D_50, D_51, B_9, R_3, D_52, P_3, B_10, D_53, S_5, B_11, S_6, D_54, R_4, S_7, B_12, S_8, D_55, D_56, B_13, R_5, D_58, S_9, B_14, D_59, D_60, D_61, B_15, S_11, D_62, D_65, B_16, B_17, B_18, B_19, B_20, S_12, R_6, S_13, B_21, D_69, B_22, D_70, D_71, D_72, S_15, B_23, D_73, P_4, D_74, D_75, D_76, B_24, R_7, D_77, B_25, B_26, D_78, D_79, R_8, R_9, S_16, D_80, R_10, R_11, B_27, D_81, D_82, S_17, R_12, B_28, R_13, D_83, R_14, R_15, D_84, R_16, B_29, S_18, D_86, D_87, R_17, R_18, D_88, B_31, S_19, R_19, B_32, S_20, R_20, R_21, B_33, D_89, R_22, R_23, D_91, D_92, D_93, D_94, R_24, R_25, D_96, S_22, S_23, S_24, S_25, S_26, D_102, D_103, D_104, D_105, D_106, D_107, B_36, B_37, R_26, R_27, D_108, D_109, D_110, D_111, B_39, D_112, B_40, S_27, D_113, D_115, D_118, D_119, D_121, D_122, D_123, D_124, D_125, D_127, D_128, D_129, B_41, B_42, D_130, D_131, D_132, D_133, R_28, D_134, D_135, D_136, D_137, D_138, D_139, D_140, D_141, D_142, D_143, D_144, D_145, B_30_CategoricalToIntegerEncoder, B_38_CategoricalToIntegerEncoder, D_114_CategoricalToIntegerEncoder, D_116_CategoricalToIntegerEncoder, D_117_CategoricalToIntegerEncoder, D_120_CategoricalToIntegerEncoder, D_126_CategoricalToIntegerEncoder, D_63_CategoricalToIntegerEncoder, D_64_CategoricalToIntegerEncoder, D_66_CategoricalToIntegerEncoder, D_68_CategoricalToIntegerEncoder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from format_data import TARGET_VARIABLE, DATE_VARIABLES, ID_VARIABLES\n",
    "\n",
    "feature_columns = [\n",
    "    c for c in train_pdf.columns \n",
    "    if c not in [TARGET_VARIABLE,] + ID_VARIABLES + list(DATE_VARIABLES.keys())\n",
    "]\n",
    "', '.join(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "[20.  1.]\n"
     ]
    }
   ],
   "source": [
    "X = train_pdf[feature_columns]\n",
    "y = train_pdf[TARGET_VARIABLE]\n",
    "print(y.unique())\n",
    "# Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "w = y.apply(lambda x: 20. if x == 1. else 1.)\n",
    "print(w.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((344184, 188), (114729, 188), (344184,), (114729,), (344184,), (114729,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, w_train, w_valid = train_test_split(X, y, w)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, w_train.shape, w_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 5.7 s, total: 47.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.955786019241124, 0.9513086169002068)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "m = LGBMClassifier().fit(\n",
    "    X=X_train, y=y_train, sample_weight=w_train,\n",
    "    categorical_feature = encs.columns_encoded\n",
    ")\n",
    "m.score(X=X_train, y=y_train, sample_weight=w_train), m.score(X=X_valid, y=y_valid, sample_weight=w_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784301470625004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import amex_metric\n",
    "from format_data import TARGET_VARIABLE, PREDICTION_VARIABLE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate(X, y_true) -> float:\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    y_pred = m.predict_proba(X)[:, 1]\n",
    "\n",
    "    y_true = pd.DataFrame({TARGET_VARIABLE: y_true})\n",
    "    y_pred = pd.DataFrame({PREDICTION_VARIABLE: y_pred})\n",
    "    return amex_metric(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "\n",
    "evaluate(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
