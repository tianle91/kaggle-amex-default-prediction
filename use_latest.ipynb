{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: Dio.netty.tryReflectionSetAccessible\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/11 15:27:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from spark_utils import get_spark_session\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = get_spark_session()\n",
    "# run rank_by_latest.py if this doesn't exist\n",
    "train_data = spark.read.parquet('data_transformed/amex-default-prediction/train_data_latest')\n",
    "train_labels = spark.read.parquet('data/amex-default-prediction/train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert train_data.count() == train_data.select('customer_ID').distinct().count()\n",
    "assert train_labels.count() == train_labels.select('customer_ID').distinct().count()\n",
    "assert train_data.count() == train_data.join(train_labels, on='customer_ID', how='inner').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from format_data import CATEGORICAL_VARIABLES\n",
    "from encoder import CategoricalToIntegerEncoders\n",
    "\n",
    "encs = CategoricalToIntegerEncoders(columns=CATEGORICAL_VARIABLES).fit(train_data)\n",
    "# not actually transformed yet, just fitted\n",
    "train_data_transformed = encs.transform(spark=spark, df=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_pdf = encs.transform(\n",
    "    spark=spark,\n",
    "    df=train_data.join(train_labels, on='customer_ID', how='inner')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_data import TARGET_VARIABLE, DATE_VARIABLES, ID_VARIABLES\n",
    "\n",
    "feature_columns = [\n",
    "    c for c in train_pdf.columns \n",
    "    if c not in [TARGET_VARIABLE,] + ID_VARIABLES + list(DATE_VARIABLES.keys())\n",
    "]\n",
    "', '.join(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_pdf[feature_columns]\n",
    "y = train_pdf[TARGET_VARIABLE]\n",
    "print(y.unique())\n",
    "# Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "w = y.apply(lambda x: 20. if x == 1. else 1.)\n",
    "print(w.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, w_train, w_valid = train_test_split(X, y, w)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, w_train.shape, w_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "m = LGBMClassifier().fit(\n",
    "    X=X_train, y=y_train, sample_weight=w_train,\n",
    "    categorical_feature = encs.columns_encoded\n",
    ")\n",
    "m.score(X=X_train, y=y_train, sample_weight=w_train), m.score(X=X_valid, y=y_valid, sample_weight=w_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import amex_metric\n",
    "from format_data import TARGET_VARIABLE, PREDICTION_VARIABLE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate(X, y_true) -> float:\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    y_pred = m.predict_proba(X)[:, 1]\n",
    "\n",
    "    y_true = pd.DataFrame({TARGET_VARIABLE: y_true})\n",
    "    y_pred = pd.DataFrame({PREDICTION_VARIABLE: y_pred})\n",
    "    return amex_metric(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "\n",
    "evaluate(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
