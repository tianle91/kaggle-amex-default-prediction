{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: Dio.netty.tryReflectionSetAccessible\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/19 03:18:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from spark_utils import get_spark_session\n",
    "\n",
    "\n",
    "spark = get_spark_session()\n",
    "# run transform_latest.py if this don't exist\n",
    "test_data = spark.read.parquet('data_transformed/amex-default-prediction/test_data_latest')\n",
    "train_data = spark.read.parquet('data_transformed/amex-default-prediction/train_data_latest')\n",
    "# run format_data.py if these don't exist\n",
    "train_labels = spark.read.parquet('data/amex-default-prediction/train_labels')\n",
    "sample_submission = spark.read.parquet('data/amex-default-prediction/sample_submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/19 03:18:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_columns\n",
      " P_2, D_39, B_1, B_2, R_1, S_3, D_41, B_3, D_42, D_43, D_44, B_4, D_45, B_5, R_2, D_46, D_47, D_48, D_49, B_6, B_7, B_8, D_50, D_51, B_9, R_3, D_52, P_3, B_10, D_53, S_5, B_11, S_6, D_54, R_4, S_7, B_12, S_8, D_55, D_56, B_13, R_5, D_58, S_9, B_14, D_59, D_60, D_61, B_15, S_11, D_62, D_65, B_16, B_17, B_18, B_19, B_20, S_12, R_6, S_13, B_21, D_69, B_22, D_70, D_71, D_72, S_15, B_23, D_73, P_4, D_74, D_75, D_76, B_24, R_7, D_77, B_25, B_26, D_78, D_79, R_8, R_9, S_16, D_80, R_10, R_11, B_27, D_81, D_82, S_17, R_12, B_28, R_13, D_83, R_14, R_15, D_84, R_16, B_29, S_18, D_86, D_87, R_17, R_18, D_88, B_31, S_19, R_19, B_32, S_20, R_20, R_21, B_33, D_89, R_22, R_23, D_91, D_92, D_93, D_94, R_24, R_25, D_96, S_22, S_23, S_24, S_25, S_26, D_102, D_103, D_104, D_105, D_106, D_107, B_36, B_37, R_26, R_27, D_108, D_109, D_110, D_111, B_39, D_112, B_40, S_27, D_113, D_115, D_118, D_119, D_121, D_122, D_123, D_124, D_125, D_127, D_128, D_129, B_41, B_42, D_130, D_131, D_132, D_133, R_28, D_134, D_135, D_136, D_137, D_138, D_139, D_140, D_141, D_142, D_143, D_144, D_145, B_30_CategoricalToIntegerEncoder, B_38_CategoricalToIntegerEncoder, D_114_CategoricalToIntegerEncoder, D_116_CategoricalToIntegerEncoder, D_117_CategoricalToIntegerEncoder, D_120_CategoricalToIntegerEncoder, D_126_CategoricalToIntegerEncoder, D_63_CategoricalToIntegerEncoder, D_64_CategoricalToIntegerEncoder, D_66_CategoricalToIntegerEncoder, D_68_CategoricalToIntegerEncoder\n",
      "CPU times: user 1.53 s, sys: 1.36 s, total: 2.88 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from format_data import CATEGORICAL_VARIABLES\n",
    "from encoder import CategoricalToIntegerEncoders\n",
    "from format_data import TARGET_VARIABLE, DATE_VARIABLES, ID_VARIABLES\n",
    "\n",
    "\n",
    "encs = CategoricalToIntegerEncoders(columns=CATEGORICAL_VARIABLES).fit(train_data)\n",
    "\n",
    "train_pdf = train_data.join(train_labels, on='customer_ID', how='inner')\n",
    "train_pdf = encs.transform(spark=spark, df=train_pdf).toPandas()\n",
    "\n",
    "test_pdf = encs.transform(spark=spark, df=test_data).toPandas()\n",
    "\n",
    "feature_columns = [\n",
    "    c for c in train_pdf.columns \n",
    "    if c not in [TARGET_VARIABLE,] + ID_VARIABLES + list(DATE_VARIABLES.keys())\n",
    "]\n",
    "print('feature_columns\\n', ', '.join(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.unique() [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((344184, 188), (114729, 188), (344184,), (114729,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_pdf[feature_columns]\n",
    "X_test = test_pdf[feature_columns]\n",
    "y = train_pdf[TARGET_VARIABLE]\n",
    "print('y.unique()', y.unique())\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(y, negative_label_weight: float = 20.):\n",
    "    # default is thus 20. because:\n",
    "    # Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "    return y.apply(lambda x: negative_label_weight if x == 0. else 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 0648c2e4f5ee42989989c14ccfe48fb0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:19:31 WARN TaskSetManager: Stage 43 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2978cb2402734c7890cf7dfa45531854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:19:55 WARN TaskSetManager: Stage 52 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: ba3604b2bd994505b1be80b33137d684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:20:20 WARN TaskSetManager: Stage 61 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: e10d4b6828964c80b52803fea9ba2877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:20:43 WARN TaskSetManager: Stage 70 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: c76a640a39b24fcf9c23495961973f04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:21:06 WARN TaskSetManager: Stage 79 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 9d589fe0533747e7a2f04fb9244ca0b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:21:30 WARN TaskSetManager: Stage 88 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 3436f72261214893a61bdfdb89ce344a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:21:53 WARN TaskSetManager: Stage 97 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 97cfd11d59274b729e15ff2df056ac74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:22:16 WARN TaskSetManager: Stage 106 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: e9189f79e2a84eeda000cfe470865921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:22:41 WARN TaskSetManager: Stage 115 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 6b2463bc29a84fa99c7d2ed0d63e537c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "22/06/19 03:23:04 WARN TaskSetManager: Stage 124 contains a task of very large size (8582 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 4s, sys: 27.6 s, total: 17min 32s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mlflow\n",
    "from lightgbm import LGBMClassifier\n",
    "from evaluation import evaluate\n",
    "import pandas as pd\n",
    "from format_data import PREDICTION_VARIABLE\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for negative_label_weight_train in np.linspace(0.01, 5., num=10, endpoint=False):\n",
    "    # mlflow.lightgbm.autolog()\n",
    "    experiment_id = mlflow.get_experiment_by_name('use_latest_tune_label_weight.ipynb').experiment_id\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f'run_id: {run_id}')\n",
    "\n",
    "        w_train = get_weights(y_train, negative_label_weight=negative_label_weight_train)\n",
    "        mlflow.log_param('negative_label_weight_train', negative_label_weight_train)\n",
    "\n",
    "        m = LGBMClassifier().fit(\n",
    "            X=X_train, y=y_train, sample_weight=w_train,\n",
    "            categorical_feature=encs.columns_encoded,\n",
    "        )\n",
    "\n",
    "        score_train = m.score(X=X_train, y=y_train, sample_weight=get_weights(y_train))\n",
    "        mlflow.log_metric('score_train', score_train)\n",
    "        score_valid = m.score(X=X_valid, y=y_valid, sample_weight=get_weights(y_valid))\n",
    "        mlflow.log_metric('score_valid', score_valid)\n",
    "        score_amex_train = evaluate(X_train, y_train, m=m)\n",
    "        mlflow.log_metric('score_amex_train', score_amex_train)\n",
    "        score_amex_valid = evaluate(X_valid, y_valid, m=m)\n",
    "        mlflow.log_metric('score_amex_valid', score_amex_valid)\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            PREDICTION_VARIABLE: m.predict_proba(X_test)[:, 1],\n",
    "            'customer_ID': test_pdf['customer_ID'],\n",
    "        })\n",
    "        pred_and_sample_joined_counts = (\n",
    "            spark\n",
    "            .createDataFrame(pred_df)\n",
    "            .join(sample_submission, on='customer_ID', how='inner')\n",
    "            .count()\n",
    "        )\n",
    "        assert pred_and_sample_joined_counts == len(pred_df), \\\n",
    "            f'''These should be identical:\n",
    "            sample_submission has {sample_submission.count()} rows,\n",
    "            pred_and_sample_joined_counts is {pred_and_sample_joined_counts},\n",
    "            pred_df has {len(pred_df)} rows\n",
    "            '''\n",
    "        with TemporaryDirectory() as p:\n",
    "            p = os.path.join(p, 'submission.csv')\n",
    "            pred_df.to_csv(p, header=True, index=False)\n",
    "            mlflow.log_artifact(local_path=p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
