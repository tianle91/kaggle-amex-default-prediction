{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_utils import get_spark_session\n",
    "\n",
    "\n",
    "spark = get_spark_session()\n",
    "# run transform_latest.py if this don't exist\n",
    "test_data = spark.read.parquet('data_transformed/amex-default-prediction/test_data_latest')\n",
    "train_data = spark.read.parquet('data_transformed/amex-default-prediction/train_data_latest')\n",
    "# run format_data.py if these don't exist\n",
    "train_labels = spark.read.parquet('data/amex-default-prediction/train_labels')\n",
    "sample_submission = spark.read.parquet('data/amex-default-prediction/sample_submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from format_data import CATEGORICAL_VARIABLES\n",
    "from encoder import CategoricalToIntegerEncoders\n",
    "from format_data import TARGET_VARIABLE, DATE_VARIABLES, ID_VARIABLES\n",
    "\n",
    "\n",
    "encs = CategoricalToIntegerEncoders(columns=CATEGORICAL_VARIABLES).fit(train_data)\n",
    "\n",
    "train_pdf = train_data.join(train_labels, on='customer_ID', how='inner')\n",
    "train_pdf = encs.transform(spark=spark, df=train_pdf).toPandas()\n",
    "\n",
    "test_pdf = encs.transform(spark=spark, df=test_data).toPandas()\n",
    "\n",
    "feature_columns = [\n",
    "    c for c in train_pdf.columns \n",
    "    if c not in [TARGET_VARIABLE,] + ID_VARIABLES + list(DATE_VARIABLES.keys())\n",
    "]\n",
    "print('feature_columns\\n', ', '.join(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_pdf[feature_columns]\n",
    "X_test = test_pdf[feature_columns]\n",
    "y = train_pdf[TARGET_VARIABLE]\n",
    "print('y.unique()', y.unique())\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(y, negative_label_weight: float = 20.):\n",
    "    # default is thus 20. because:\n",
    "    # Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "    return y.apply(lambda x: negative_label_weight if x == 0. else 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import mlflow\n",
    "from lightgbm import LGBMClassifier\n",
    "from evaluation import evaluate\n",
    "import pandas as pd\n",
    "from format_data import PREDICTION_VARIABLE\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for negative_label_weight_train in np.linspace(0.01, 20., num=10, endpoint=False):\n",
    "    # mlflow.lightgbm.autolog()\n",
    "    experiment_id = mlflow.get_experiment_by_name('use_latest_tune_label_weight.ipynb').experiment_id\n",
    "    with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f'run_id: {run_id}')\n",
    "\n",
    "        w_train = get_weights(y_train, negative_label_weight=negative_label_weight_train)\n",
    "        mlflow.log_param('negative_label_weight_train', negative_label_weight_train)\n",
    "\n",
    "        m = LGBMClassifier().fit(\n",
    "            X=X_train, y=y_train, sample_weight=w_train,\n",
    "            categorical_feature=encs.columns_encoded,\n",
    "        )\n",
    "\n",
    "        score_train = m.score(X=X_train, y=y_train, sample_weight=get_weights(y_train))\n",
    "        mlflow.log_metric('score_train', score_train)\n",
    "        score_valid = m.score(X=X_valid, y=y_valid, sample_weight=get_weights(y_valid))\n",
    "        mlflow.log_metric('score_valid', score_valid)\n",
    "        score_amex_train = evaluate(X_train, y_train, m=m)\n",
    "        mlflow.log_metric('score_amex_train', score_amex_train)\n",
    "        score_amex_valid = evaluate(X_valid, y_valid, m=m)\n",
    "        mlflow.log_metric('score_amex_valid', score_amex_valid)\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            PREDICTION_VARIABLE: m.predict_proba(X_test)[:, 1],\n",
    "            'customer_ID': test_pdf['customer_ID'],\n",
    "        })\n",
    "        pred_and_sample_joined_counts = (\n",
    "            spark\n",
    "            .createDataFrame(pred_df)\n",
    "            .join(sample_submission, on='customer_ID', how='inner')\n",
    "            .count()\n",
    "        )\n",
    "        assert pred_and_sample_joined_counts == len(pred_df), \\\n",
    "            f'''These should be identical:\n",
    "            sample_submission has {sample_submission.count()} rows,\n",
    "            pred_and_sample_joined_counts is {pred_and_sample_joined_counts},\n",
    "            pred_df has {len(pred_df)} rows\n",
    "            '''\n",
    "        with TemporaryDirectory() as p:\n",
    "            p = os.path.join(p, 'submission.csv')\n",
    "            pred_df.to_csv(p, header=True, index=False)\n",
    "            mlflow.log_artifact(local_path=p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
